{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0248de-4a08-47e8-a8d5-2861d34e6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc99af6b-c638-4100-8606-a6c786afd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "trainingset = pd.read_csv('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/selectdata_reduce_faint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0260743-8d09-4f2f-849a-ea5dbe6410ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"stamp_train = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/stamp_train.npy')\\nmask_train = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/mask_train.npy')\\n\\nmask_gal = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/mask_gal.npy')\\nmask_annulus = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/mask_annulus.npy')\\narea_annulus = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/area_annulus.npy')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''stamp_train = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/stamp_train.npy')\n",
    "mask_train = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/mask_train.npy')\n",
    "\n",
    "mask_gal = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/mask_gal.npy')\n",
    "mask_annulus = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/mask_annulus.npy')\n",
    "area_annulus = np.load('/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/area_annulus.npy')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805d628f-1cb2-49dc-9530-f1747823f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The class to arrange the lines of the dataframe\n",
    "#Can see the below comments\n",
    "\n",
    "class PairedBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    choose P ref_ids, and choose 2 from each ref_id.\n",
    "    \n",
    "    P = 8, K = 2 , batch_size = 16\n",
    "    batch_indices = [idx_A1, idx_G1, ... idx_F1,   idx_A2, idx_G2, ... idx_F2]\n",
    "                  \n",
    "    - ref_ids (list)\n",
    "    - P (int): number of the pair\n",
    "    \"\"\"\n",
    "    def __init__(self, ref_ids, P):\n",
    "        super(PairedBatchSampler, self).__init__()\n",
    "        \n",
    "        if P <= 0:\n",
    "            raise ValueError(\"P must be > 0\")\n",
    "\n",
    "        self.P = P\n",
    "        self.K_is_fixed_at = 2\n",
    "        self.batch_size = P * self.K_is_fixed_at\n",
    "        \n",
    "        print(\"constructing PairedBatchSampler ...\")\n",
    "        grouped_indices = defaultdict(list)\n",
    "        for i, ref_id in enumerate(ref_ids):\n",
    "            grouped_indices[ref_id].append(i)\n",
    "        \n",
    "        print(f\"creating 'chunks' (size K={self.K_is_fixed_at})...\")\n",
    "        self.all_chunks = []\n",
    "        for ref_id, indices in grouped_indices.items():\n",
    "            if len(indices) >= self.K_is_fixed_at: #Acutally I have already selected the ref_ids\n",
    "                \n",
    "                random.shuffle(indices) #make it random\n",
    "                \n",
    "                # divide the ref_id. e.g. floor(13 / 4) = 3. we build 3 chunks。\n",
    "                num_chunks_for_this_id = len(indices) // self.K_is_fixed_at\n",
    "                \n",
    "                for i in range(num_chunks_for_this_id):\n",
    "                    chunk = indices[i * self.K_is_fixed_at : (i + 1) * self.K_is_fixed_at]\n",
    "                    self.all_chunks.append(chunk)\n",
    "        \n",
    "        print(f\"Already created {len(self.all_chunks)} 'K-chunks'.\")\n",
    "        \n",
    "        if len(self.all_chunks) < P:\n",
    "            raise ValueError(f\"ref_ids are less than P={P} 'K-chunks'. Use a smaller K or P\")\n",
    "            \n",
    "        self.num_batches = len(self.all_chunks) // P\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        random.shuffle(self.all_chunks)\n",
    "        \n",
    "        for i in range(self.num_batches):\n",
    "            batch_part1_indices = []\n",
    "            batch_part2_indices = []\n",
    "            \n",
    "            p_chunks = self.all_chunks[i * self.P : (i + 1) * self.P]\n",
    "            \n",
    "            for chunk in p_chunks:\n",
    "                \n",
    "                batch_part1_indices.append(chunk[0])\n",
    "                batch_part2_indices.append(chunk[1])\n",
    "            \n",
    "            final_batch_indices = batch_part1_indices + batch_part2_indices\n",
    "            \n",
    "            # P + P = 16\n",
    "            yield final_batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f44423c-383b-47e7-b6ae-573edb7e81e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing PairedBatchSampler ...\n",
      "creating 'chunks' (size K=2)...\n",
      "Already created 14629 'K-chunks'.\n",
      "\n",
      " getting index from the sampler...\n",
      "Have created 29248 lines\n"
     ]
    }
   ],
   "source": [
    "#Select and arrange the dataframe\n",
    "P = 8\n",
    "sampler = PairedBatchSampler(trainingset['ref_id'], P=P)\n",
    "print(\"\\n getting index from the sampler...\")\n",
    "list_of_all_batches = list(sampler)\n",
    "\n",
    "shuffled_indices = [index for batch in list_of_all_batches for index in batch]\n",
    "print(f\"Have created {len(shuffled_indices)} lines\")\n",
    "\n",
    "shuffled_df_train = trainingset.iloc[shuffled_indices]\n",
    "shuffled_df_train = shuffled_df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6df7b6-591d-425b-b74a-99ed8a4a627b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#generate image\n",
    "cutout_size = 48\n",
    "\n",
    "def background_annulus(data, mask, aperture_x, aperture_y, r_in=30, r_out=45):\n",
    "    \"\"\"Measure background in an annulus.\"\"\"\n",
    "    \n",
    "    masked_data = np.ma.array(data=data, mask=mask != 0)\n",
    "    masked_data = masked_data.filled(fill_value=0)\n",
    "\n",
    "    center = (aperture_x, aperture_y)\n",
    "    annulus_apertures = CircularAnnulus(center, r_in=r_in, r_out=r_out)\n",
    "    masks = annulus_apertures.to_mask(method='center')\n",
    "\n",
    "    cutout_data = masks.cutout(masked_data)\n",
    "\n",
    "    clip_annulus_array = sigma_clip(cutout_data[cutout_data != 0], sigma=3, maxiters=2)\n",
    "\n",
    "    S = pd.Series()\n",
    "    S['annulus_mean'] = np.ma.mean(clip_annulus_array)\n",
    "    S['annulus_median'] = np.ma.median(clip_annulus_array)\n",
    "    S['annulus_std'] = np.ma.std(clip_annulus_array)\n",
    "    S['annulus_samples'] = np.ma.count(clip_annulus_array)\n",
    "\n",
    "    return S\n",
    "\n",
    "def flux_elliptical(image, mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b):\n",
    "    \"\"\"Measure the flux withing an elliptical aperture.\"\"\"\n",
    "    \n",
    "    PIXEL_SCALE = 0.263\n",
    "    theta = -aperture_theta * np.pi / 180.\n",
    "    a = aperture_a / PIXEL_SCALE\n",
    "    b = aperture_b / PIXEL_SCALE\n",
    "\n",
    "    center = (aperture_x, aperture_y)\n",
    "    source_aperture = EllipticalAperture(center, a, b, theta)\n",
    "\n",
    "    xmask = mask != 0\n",
    "    raw_flux = aperture_photometry(image, source_aperture, mask=xmask)\n",
    "   \n",
    "    S = pd.Series()\n",
    "    S['raw_flux'] = float(raw_flux['aperture_sum'][0])\n",
    "    S['area'] = source_aperture.area\n",
    "    \n",
    "    return S\n",
    "\n",
    "def cal_calerror(sig_src,sig_zp,zp,f_src):\n",
    "    sig_cal = np.sqrt(sig_src**2 * sig_zp**2 + sig_src**2 * zp**2 + sig_zp**2 * f_src**2)\n",
    "    return sig_cal\n",
    "\n",
    "def cal_fcal(f_src,zp):\n",
    "    f_cal = f_src * zp\n",
    "    return f_cal\n",
    "\n",
    "def creat_stamps(image, sources):\n",
    "    y = math.floor(sources['aperture_x'].values[0])\n",
    "    x = math.floor(sources['aperture_y'].values[0])\n",
    "    #y = math.floor(sources['aperture_x'])\n",
    "    #x = math.floor(sources['aperture_y'])\n",
    "    x_start = max((x - cutout_size), 0)\n",
    "    x_end = min((x + cutout_size), image.shape[0])\n",
    "    y_start = max((y - cutout_size), 0)\n",
    "    y_end = min((y + cutout_size), image.shape[1])\n",
    "\n",
    "    stamps = image[x_start:x_end, y_start:y_end]\n",
    "    return stamps\n",
    "\n",
    "def photometry_oneimage(image, mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b):\n",
    "    \n",
    "    S1 = background_annulus(image, mask, aperture_x, aperture_y)\n",
    "    S2 = flux_elliptical(image, mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b)\n",
    "\n",
    "    flux_obs = S2['raw_flux'] - S2['area'] * S1['annulus_mean']\n",
    "    return flux_obs, S1['annulus_std']\n",
    "\n",
    "def generate_mask(size_data, image):\n",
    "\n",
    "    num_sample = int(size_data[0] * size_data[1] * (1 - ratio))\n",
    "    mask = np.ones(size_data)\n",
    "    output = image\n",
    "\n",
    "    for ich in range(size_data[2]):\n",
    "        idy_msk = np.random.randint(0, size_data[0], num_sample)\n",
    "        idx_msk = np.random.randint(0, size_data[1], num_sample)\n",
    "\n",
    "        idy_neigh = np.random.randint(-size_window[0] // 2 + size_window[0] % 2, size_window[0] // 2 + size_window[0] % 2, num_sample)\n",
    "        idx_neigh = np.random.randint(-size_window[1] // 2 + size_window[1] % 2, size_window[1] // 2 + size_window[1] % 2, num_sample)\n",
    "\n",
    "        idy_msk_neigh = idy_msk + idy_neigh\n",
    "        idx_msk_neigh = idx_msk + idx_neigh\n",
    "\n",
    "        idy_msk_neigh = idy_msk_neigh + (idy_msk_neigh < 0) * size_data[0] - (idy_msk_neigh >= size_data[0]) * size_data[0]\n",
    "        idx_msk_neigh = idx_msk_neigh + (idx_msk_neigh < 0) * size_data[1] - (idx_msk_neigh >= size_data[1]) * size_data[1]\n",
    "\n",
    "        id_msk = (idy_msk, idx_msk, ich)\n",
    "        id_msk_neigh = (idy_msk_neigh, idx_msk_neigh, ich)\n",
    "\n",
    "        output[id_msk] = image[id_msk_neigh]\n",
    "        mask[id_msk] = 0.0\n",
    "\n",
    "    return output, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234ecc4c-61e8-470c-b791-8561dfbe8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the function to generate the mask_annulus, mask_gal\n",
    "#test the code in the following\n",
    "def annulus_mask_generator(mask, aperture_x, aperture_y, r_in=30, r_out=45):\n",
    "\n",
    "    image_shape = (cutout_size*2,cutout_size*2)\n",
    "    center = (aperture_x, aperture_y)\n",
    "    annulus_apertures = CircularAnnulus(center, r_in=r_in, r_out=r_out)\n",
    "    \n",
    "    mask_object = annulus_apertures.to_mask(method='center')\n",
    "    mask_annulus = mask_object.to_image(shape=image_shape)\n",
    "    \n",
    "    xmask = mask != 0\n",
    "    \n",
    "    mask_annulus = mask_annulus * (1 - xmask)\n",
    "    \n",
    "    return mask_annulus, annulus_apertures.area\n",
    "\n",
    "def gal_mask_generator(mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b):\n",
    "\n",
    "    image_shape = (cutout_size*2,cutout_size*2)\n",
    "    PIXEL_SCALE = 0.263\n",
    "    theta = -aperture_theta * np.pi / 180.\n",
    "    a = aperture_a / PIXEL_SCALE\n",
    "    b = aperture_b / PIXEL_SCALE\n",
    "\n",
    "    center = (aperture_x, aperture_y)\n",
    "    source_aperture = EllipticalAperture(center, a, b, theta)\n",
    "    mask_object = source_aperture.to_mask(method='exact')\n",
    "    mask_image_photutils_fractional = mask_object.to_image(shape=image_shape)\n",
    "    \n",
    "    xmask = mask != 0\n",
    "    mask_gal = mask_image_photutils_fractional * (1 - xmask)\n",
    "    \n",
    "    return mask_gal#, source_aperture.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff15f243-0f7f-4db9-9501-3f05897f2a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29248/29248 [37:58<00:00, 12.84it/s]  \n"
     ]
    }
   ],
   "source": [
    "#generate the stamp and mask\n",
    "cutout_size = 48\n",
    "shuffled_stamp_train = np.zeros((len(shuffled_df_train), cutout_size*2, cutout_size*2))\n",
    "shuffled_mask_train = np.zeros((len(shuffled_df_train), cutout_size*2, cutout_size*2))\n",
    "\n",
    "for i in tqdm(range(len(shuffled_df_train))):\n",
    "    row = shuffled_df_train.iloc[[i]]\n",
    "    path = shuffled_df_train['path'][i]\n",
    "    image = fits.getdata(path)\n",
    "    mask = fits.getdata(path.replace('.fits', '.mask.fits'))\n",
    "    \n",
    "    shuffled_stamp_train[i] = creat_stamps(image, row)\n",
    "    shuffled_mask_train[i] = creat_stamps(mask, row)\n",
    "\n",
    "\n",
    "shuffled_mask_gal = np.zeros((len(shuffled_df_train), cutout_size*2, cutout_size*2))\n",
    "shuffled_mask_annulus = np.zeros((len(shuffled_df_train), cutout_size*2, cutout_size*2))\n",
    "shuffled_area_annulus = np.zeros((len(shuffled_df_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff23448-f87b-49b4-a655-28c07db9f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29248/29248 [01:01<00:00, 474.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(shuffled_df_train))):\n",
    "    row = shuffled_df_train.iloc[[i]]\n",
    "    aperture_x = cutout_size + row['aperture_x'].item() - math.floor(row['aperture_x'].item())\n",
    "    aperture_y = cutout_size + row['aperture_y'].item() - math.floor(row['aperture_y'].item())\n",
    "\n",
    "    shuffled_mask_gal[i] = gal_mask_generator(shuffled_mask_train[i], aperture_x, aperture_y,\n",
    "                                   row['aperture_theta'].item(),row['aperture_a'].item(),row['aperture_b'].item())\n",
    "    shuffled_mask_annulus[i], shuffled_area_annulus[i] = annulus_mask_generator(shuffled_mask_train[i], aperture_x, aperture_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b366fa-11a6-4db3-bed4-b6ddf2427b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part need to process the mask_abnormal,to (1 - mask_abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14b5647e-d62e-4f72-a379-855da5bd7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_mask_train = 1 - shuffled_mask_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d348c067-92e4-4ba3-af0f-bc6bf8325422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert them into tensor\n",
    "tensor_stamp_train = torch.from_numpy(shuffled_stamp_train).float()\n",
    "tensor_mask_train = torch.from_numpy(shuffled_mask_train).float()\n",
    "tensor_mask_gal = torch.from_numpy(shuffled_mask_gal).float()\n",
    "tensor_mask_annulus = torch.from_numpy(shuffled_mask_annulus).float()\n",
    "tensor_area_annulus = torch.from_numpy(shuffled_area_annulus).float()\n",
    "\n",
    "tensor_stamp_train = tensor_stamp_train.unsqueeze(1)\n",
    "tensor_mask_train = tensor_mask_train.unsqueeze(1)\n",
    "tensor_mask_gal = tensor_mask_gal.unsqueeze(1)\n",
    "tensor_mask_annulus = tensor_mask_annulus.unsqueeze(1)\n",
    "tensor_area_annulus = tensor_area_annulus.unsqueeze(1)\n",
    "\n",
    "#We just need 'ref_id', 'zp'\n",
    "features_df_train = shuffled_df_train[['area', 'ref_id', 'zp']].values\n",
    "features_df_train_tensor = torch.FloatTensor(features_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea680241-9002-453a-bf62-19d0ca1c6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(tensor_stamp_train, tensor_mask_train, tensor_mask_gal, tensor_mask_annulus, tensor_area_annulus, features_df_train_tensor)\n",
    "traindataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "#I set shuffle=False here, because I have already do the shuffle before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606d8a4-1b6e-4990-8ba9-f941e1ce73d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056a4d1-d202-421a-9a0c-a9ee494d379c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d8011a1-ec84-47ea-a107-a475d08cfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have already simplified the loss functions\n",
    "\n",
    "def loss_unbias(flux_gal_inputs, flux_gal_outputs):\n",
    "\n",
    "    loss = (flux_gal_inputs - flux_gal_outputs).abs().sum()\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "def PairedDifferenceLoss(flux_gal_calibrated_outputs):\n",
    "    \n",
    "    batch_size = flux_gal_calibrated_outputs.shape[0]\n",
    "    half_B = batch_size // 2 #divide into 2\n",
    "        \n",
    "    outputs_1 = flux_gal_calibrated_outputs[0:half_B]\n",
    "    outputs_2 = flux_gal_calibrated_outputs[half_B:]\n",
    "    loss = (outputs_1 - outputs_2).abs().sum()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3297a0fa-e74a-424b-8c56-8f75fd826c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "#load the pn2v model\n",
    "import os\n",
    "os.chdir('/data/aai/scratch/jchan/denoise/PAUS/dinggetest/simulation/pn2v/src/pn2v')\n",
    "from core import prediction\n",
    "from core import utils\n",
    "from unet import UNet\n",
    "\n",
    "device=utils.getDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9cfb239-8881-411e-aa5b-750eb2f3d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63/4130286196.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model=torch.load(path+\"/best_conv_N2V_PAUdm.net\")\n"
     ]
    }
   ],
   "source": [
    "path='/data/aai/scratch/jchan/denoise/PAUS/dinggetest/simulation/model saved/'\n",
    "model=torch.load(path+\"/best_conv_N2V_PAUdm.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f17d729-5e48-4b2a-8070-8ff58cfa7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd5c5867-0dbd-4e72-83c8-a911b5322798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)\n",
    "target_list = [\n",
    "    'conv_final',\n",
    "    'down_convs.0.conv1',\n",
    "    'down_convs.0.conv2',\n",
    "    'down_convs.1.conv1',\n",
    "    'down_convs.1.conv2',\n",
    "    'down_convs.2.conv1',\n",
    "    'down_convs.2.conv2',\n",
    "    'up_convs.0.conv1',\n",
    "    'up_convs.0.conv2',\n",
    "    'up_convs.1.conv1',\n",
    "    'up_convs.1.conv2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67b6b4ed-fb81-4239-bffa-d439f17208a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 84,560 || all params: 1,761,938 || trainable%: 4.7993\n"
     ]
    }
   ],
   "source": [
    "#I add some Chinese comments here to explain some parameters...\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,  #它控制了 LoRA 模块的“大小”或“复杂度”。r 越大，LoRA 模块的可训练参数就越多，理论上能学习更复杂的调整，但也会占用更多显存。r=8 或 16 是一个非常常见且高效的选择。\n",
    "    lora_alpha=16, #LoRA 的输出会乘以一个缩放比例 alpha/r。这就像一个特殊的“学习率”或“平衡旋钮”。一个常见的经验法则是将 lora_alpha 设置为 r 的两倍（比如 r=8, alpha=16），这有助于稳定训练。\n",
    "    target_modules=target_list,\n",
    "    lora_dropout=0.1,#在 LoRA 模块中添加一个 Dropout 层，用于防止过拟合，这是一个标准的正则化技术。To prevent overfitting\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c94919ef-9b08-4b29-85cb-436e8a89896c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.11it/s, Loss of this batch=3.681190, Avg Loss=0.002014] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Completed - Average unbias Loss: 2.363595 Average df Loss 1.034137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.03it/s, Loss of this batch=3.599239, Avg Loss=0.001969] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Completed - Average unbias Loss: 2.176764 Average df Loss 0.983623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.20it/s, Loss of this batch=3.744357, Avg Loss=0.002048] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Completed - Average unbias Loss: 2.180364 Average df Loss 0.835022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.44it/s, Loss of this batch=3.445913, Avg Loss=0.001885] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Completed - Average unbias Loss: 2.183434 Average df Loss 0.799905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.08it/s, Loss of this batch=3.369607, Avg Loss=0.001843] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Completed - Average unbias Loss: 2.178540 Average df Loss 0.780810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200: 100%|██████████| 1828/1828 [01:03<00:00, 28.84it/s, Loss of this batch=3.524887, Avg Loss=0.001928] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Completed - Average unbias Loss: 2.162814 Average df Loss 0.773696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200: 100%|██████████| 1828/1828 [01:03<00:00, 28.86it/s, Loss of this batch=3.519748, Avg Loss=0.001925] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Completed - Average unbias Loss: 2.132239 Average df Loss 0.785664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.38it/s, Loss of this batch=3.511632, Avg Loss=0.001921] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Completed - Average unbias Loss: 2.139799 Average df Loss 0.774789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.24it/s, Loss of this batch=3.550143, Avg Loss=0.001942] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Completed - Average unbias Loss: 2.134182 Average df Loss 0.770952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200: 100%|██████████| 1828/1828 [01:02<00:00, 29.23it/s, Loss of this batch=3.485407, Avg Loss=0.001907] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Completed - Average unbias Loss: 2.133935 Average df Loss 0.765080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200:  41%|████▏     | 758/1828 [00:25<00:36, 29.18it/s, Loss of this batch=2.047821, Avg Loss=0.002702]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 43\u001b[0m current_loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mloss1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m current_loss2 \u001b[38;5;241m=\u001b[39m loss2\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     45\u001b[0m total_loss1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_loss1\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "lora_model.train()\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss1 = 0.0\n",
    "    total_loss2 = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(traindataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    for tensor_stamp_train, tensor_mask_train, tensor_mask_gal, tensor_mask_annulus, tensor_area_annulus, features_df_train_tensor in progress_bar:\n",
    "\n",
    "        inputs = tensor_stamp_train.to(device)\n",
    "        mask_abnormal = tensor_mask_train.to(device) \n",
    "        mask_gal = tensor_mask_gal.to(device)\n",
    "        mask_annulus = tensor_mask_annulus.to(device)\n",
    "        area_annulus = tensor_area_annulus.to(device)\n",
    "        features = features_df_train_tensor.to(device)#'area', 'ref_id', 'zp'\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = lora_model(inputs) \n",
    "        #batch_size = inputs.shape[0]\n",
    "        batch_size = inputs.shape[0]\n",
    "        #get the flux\n",
    "        flux_raw_inputs = torch.sum(inputs * mask_gal * mask_abnormal, dim=[2, 3])\n",
    "        flux_raw_outputs = torch.sum(outputs * mask_gal * mask_abnormal, dim=[2, 3])\n",
    "        flux_annulus_inputs = torch.sum(inputs * mask_annulus, dim=[2, 3])\n",
    "        flux_annulus_outputs = torch.sum(outputs * mask_annulus, dim=[2, 3])\n",
    "        flux_gal_inputs = flux_raw_inputs - flux_annulus_inputs * features[:,0].unsqueeze(1) / area_annulus\n",
    "        flux_gal_outputs = flux_raw_outputs - flux_annulus_outputs * features[:,0].unsqueeze(1) / area_annulus\n",
    "        #get the calibrated flux.\n",
    "        #Note that here we just calculate the calibrated flux of the denoised image, we don't need to include the undenoised ones\n",
    "        flux_gal_calibrated_outputs = flux_gal_outputs * features[:,2].unsqueeze(1)\n",
    "        \n",
    "        loss1 = loss_unbias(flux_gal_inputs, flux_gal_outputs) / batch_size\n",
    "        loss2 = PairedDifferenceLoss(flux_gal_calibrated_outputs) / batch_size#(batch_size/2)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss1 = loss1.item()\n",
    "        current_loss2 = loss2.item()\n",
    "        total_loss1 += current_loss1\n",
    "        total_loss2 += current_loss2\n",
    "        num_batches += 1\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss of this batch': f'{current_loss1 + current_loss2:.6f}',\n",
    "            'Avg Loss': f'{(current_loss1+current_loss2)/num_batches:.6f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss1 = total_loss1 / num_batches\n",
    "    avg_loss2 = total_loss2 / num_batches\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Completed - Average unbias Loss: {avg_loss1:.6f} Average df Loss {avg_loss2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b646b790-27f2-4b29-9b95-f207b1a70b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/data/aai/scratch/jchan/denoise/PAUS/output_save/modify_pn2v/saved_models\"\n",
    "final_model_path = os.path.join(save_dir, \"lora_model_final_faint3.pth\")\n",
    "torch.save(lora_model, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419a608-2c76-4bae-9d7d-c1f7b0c04ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ca958-6938-4a11-a001-78694cf59fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd079aa-10cb-4040-9973-8e4be0bbccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddba3d-425b-4b51-9c82-4bc5833c0745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd24033-e1e7-46b6-97ef-b6f6baa99c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6241f-6d4c-4cb2-85d5-a4529ec3f8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e51ed72d-277e-42ae-9ca0-f375f6c0d3c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.66it/s, Loss of this batch=2.400738, Avg Loss=0.001313] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Completed - Average unbias Loss: 2.505855 Average df Loss 0.993405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.64it/s, Loss of this batch=2.396783, Avg Loss=0.001311] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Completed - Average unbias Loss: 2.202726 Average df Loss 1.097579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.63it/s, Loss of this batch=2.488276, Avg Loss=0.001361] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Completed - Average unbias Loss: 2.192999 Average df Loss 1.090032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.75it/s, Loss of this batch=2.287490, Avg Loss=0.001251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Completed - Average unbias Loss: 2.188943 Average df Loss 1.080061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.68it/s, Loss of this batch=2.485907, Avg Loss=0.001360] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Completed - Average unbias Loss: 2.175875 Average df Loss 1.060213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.52it/s, Loss of this batch=2.468414, Avg Loss=0.001350] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Completed - Average unbias Loss: 2.152380 Average df Loss 0.955716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.63it/s, Loss of this batch=2.348297, Avg Loss=0.001285] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Completed - Average unbias Loss: 2.165776 Average df Loss 0.889056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.45it/s, Loss of this batch=2.310762, Avg Loss=0.001264] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Completed - Average unbias Loss: 2.173780 Average df Loss 0.863677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.24it/s, Loss of this batch=2.184701, Avg Loss=0.001195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Completed - Average unbias Loss: 2.173960 Average df Loss 0.845279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.13it/s, Loss of this batch=2.110257, Avg Loss=0.001154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Completed - Average unbias Loss: 2.176570 Average df Loss 0.833009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.83it/s, Loss of this batch=2.020530, Avg Loss=0.001105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 Completed - Average unbias Loss: 2.181102 Average df Loss 0.819390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.78it/s, Loss of this batch=2.077083, Avg Loss=0.001136] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Completed - Average unbias Loss: 2.183558 Average df Loss 0.810261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200: 100%|██████████| 1828/1828 [00:58<00:00, 30.99it/s, Loss of this batch=2.009910, Avg Loss=0.001100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Completed - Average unbias Loss: 2.174234 Average df Loss 0.810476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.00it/s, Loss of this batch=1.949056, Avg Loss=0.001066] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Completed - Average unbias Loss: 2.172294 Average df Loss 0.804044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.88it/s, Loss of this batch=1.977847, Avg Loss=0.001082] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Completed - Average unbias Loss: 2.173094 Average df Loss 0.797639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.12it/s, Loss of this batch=2.051170, Avg Loss=0.001122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 Completed - Average unbias Loss: 2.173218 Average df Loss 0.793966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.74it/s, Loss of this batch=2.061902, Avg Loss=0.001128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 Completed - Average unbias Loss: 2.175543 Average df Loss 0.785703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.09it/s, Loss of this batch=2.099666, Avg Loss=0.001149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 Completed - Average unbias Loss: 2.175127 Average df Loss 0.781405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.33it/s, Loss of this batch=1.977418, Avg Loss=0.001082] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 Completed - Average unbias Loss: 2.171512 Average df Loss 0.777210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.37it/s, Loss of this batch=1.995524, Avg Loss=0.001092] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 Completed - Average unbias Loss: 2.172969 Average df Loss 0.769213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.97it/s, Loss of this batch=2.020535, Avg Loss=0.001105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 Completed - Average unbias Loss: 2.164102 Average df Loss 0.768847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.21it/s, Loss of this batch=1.951420, Avg Loss=0.001068] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 Completed - Average unbias Loss: 2.161699 Average df Loss 0.770451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.38it/s, Loss of this batch=1.998374, Avg Loss=0.001093] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 Completed - Average unbias Loss: 2.152087 Average df Loss 0.771088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.49it/s, Loss of this batch=1.945724, Avg Loss=0.001064] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 Completed - Average unbias Loss: 2.147205 Average df Loss 0.770479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.52it/s, Loss of this batch=1.982602, Avg Loss=0.001085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 Completed - Average unbias Loss: 2.147891 Average df Loss 0.764959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.90it/s, Loss of this batch=2.017751, Avg Loss=0.001104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 Completed - Average unbias Loss: 2.142851 Average df Loss 0.767157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.70it/s, Loss of this batch=2.042185, Avg Loss=0.001117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 Completed - Average unbias Loss: 2.140498 Average df Loss 0.765940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200: 100%|██████████| 1828/1828 [01:00<00:00, 30.33it/s, Loss of this batch=2.043180, Avg Loss=0.001118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 Completed - Average unbias Loss: 2.140147 Average df Loss 0.761125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.85it/s, Loss of this batch=1.953498, Avg Loss=0.001069] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 Completed - Average unbias Loss: 2.134289 Average df Loss 0.764280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.98it/s, Loss of this batch=1.970421, Avg Loss=0.001078] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 Completed - Average unbias Loss: 2.134346 Average df Loss 0.762606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.93it/s, Loss of this batch=1.974146, Avg Loss=0.001080] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 Completed - Average unbias Loss: 2.137278 Average df Loss 0.757796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.94it/s, Loss of this batch=2.029122, Avg Loss=0.001110] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 Completed - Average unbias Loss: 2.133624 Average df Loss 0.761005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.76it/s, Loss of this batch=1.925510, Avg Loss=0.001053] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 Completed - Average unbias Loss: 2.131213 Average df Loss 0.758270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.27it/s, Loss of this batch=2.002822, Avg Loss=0.001096] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 Completed - Average unbias Loss: 2.133073 Average df Loss 0.755497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.56it/s, Loss of this batch=1.905852, Avg Loss=0.001043] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 Completed - Average unbias Loss: 2.132546 Average df Loss 0.755278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.62it/s, Loss of this batch=1.955257, Avg Loss=0.001070] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 Completed - Average unbias Loss: 2.130243 Average df Loss 0.754262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.53it/s, Loss of this batch=1.970322, Avg Loss=0.001078] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 Completed - Average unbias Loss: 2.130683 Average df Loss 0.752534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.45it/s, Loss of this batch=2.044731, Avg Loss=0.001119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 Completed - Average unbias Loss: 2.133041 Average df Loss 0.748108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.63it/s, Loss of this batch=1.992592, Avg Loss=0.001090] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 Completed - Average unbias Loss: 2.130786 Average df Loss 0.746988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.67it/s, Loss of this batch=2.068606, Avg Loss=0.001132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 Completed - Average unbias Loss: 2.126901 Average df Loss 0.749322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.69it/s, Loss of this batch=1.959789, Avg Loss=0.001072] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41 Completed - Average unbias Loss: 2.130740 Average df Loss 0.746527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.25it/s, Loss of this batch=2.050347, Avg Loss=0.001122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 Completed - Average unbias Loss: 2.125679 Average df Loss 0.750163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.70it/s, Loss of this batch=1.930385, Avg Loss=0.001056] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43 Completed - Average unbias Loss: 2.128901 Average df Loss 0.746686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.17it/s, Loss of this batch=2.161511, Avg Loss=0.001182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 Completed - Average unbias Loss: 2.129749 Average df Loss 0.743747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.06it/s, Loss of this batch=2.011214, Avg Loss=0.001100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 Completed - Average unbias Loss: 2.128418 Average df Loss 0.742175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.17it/s, Loss of this batch=2.013753, Avg Loss=0.001102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 Completed - Average unbias Loss: 2.125126 Average df Loss 0.742115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.13it/s, Loss of this batch=2.056700, Avg Loss=0.001125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47 Completed - Average unbias Loss: 2.128567 Average df Loss 0.739746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.75it/s, Loss of this batch=1.966167, Avg Loss=0.001076] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 Completed - Average unbias Loss: 2.128693 Average df Loss 0.742254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.75it/s, Loss of this batch=2.040250, Avg Loss=0.001116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 Completed - Average unbias Loss: 2.124749 Average df Loss 0.741855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.20it/s, Loss of this batch=2.022845, Avg Loss=0.001107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 Completed - Average unbias Loss: 2.125592 Average df Loss 0.742921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.00it/s, Loss of this batch=1.961495, Avg Loss=0.001073] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51 Completed - Average unbias Loss: 2.124761 Average df Loss 0.740927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.03it/s, Loss of this batch=1.930691, Avg Loss=0.001056] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52 Completed - Average unbias Loss: 2.124585 Average df Loss 0.741346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.02it/s, Loss of this batch=1.990157, Avg Loss=0.001089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53 Completed - Average unbias Loss: 2.126825 Average df Loss 0.738956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.08it/s, Loss of this batch=1.972989, Avg Loss=0.001079] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54 Completed - Average unbias Loss: 2.123230 Average df Loss 0.741403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.32it/s, Loss of this batch=1.897880, Avg Loss=0.001038] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55 Completed - Average unbias Loss: 2.124494 Average df Loss 0.739622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.24it/s, Loss of this batch=1.927301, Avg Loss=0.001054] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 56 Completed - Average unbias Loss: 2.129264 Average df Loss 0.733663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.97it/s, Loss of this batch=1.991717, Avg Loss=0.001090] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57 Completed - Average unbias Loss: 2.126446 Average df Loss 0.734070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.12it/s, Loss of this batch=1.990403, Avg Loss=0.001089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58 Completed - Average unbias Loss: 2.129357 Average df Loss 0.732440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.40it/s, Loss of this batch=1.949899, Avg Loss=0.001067] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59 Completed - Average unbias Loss: 2.125531 Average df Loss 0.736046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.50it/s, Loss of this batch=2.009375, Avg Loss=0.001099] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60 Completed - Average unbias Loss: 2.124889 Average df Loss 0.734070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.53it/s, Loss of this batch=1.992555, Avg Loss=0.001090] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61 Completed - Average unbias Loss: 2.125806 Average df Loss 0.731681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.32it/s, Loss of this batch=2.132642, Avg Loss=0.001167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62 Completed - Average unbias Loss: 2.126598 Average df Loss 0.730824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.38it/s, Loss of this batch=2.020654, Avg Loss=0.001105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63 Completed - Average unbias Loss: 2.123690 Average df Loss 0.733471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.92it/s, Loss of this batch=1.950217, Avg Loss=0.001067] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64 Completed - Average unbias Loss: 2.127718 Average df Loss 0.731659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/200: 100%|██████████| 1828/1828 [00:59<00:00, 30.86it/s, Loss of this batch=1.984007, Avg Loss=0.001085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 159 Completed - Average unbias Loss: 2.094294 Average df Loss 0.700940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.16it/s, Loss of this batch=2.003918, Avg Loss=0.001096] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 160 Completed - Average unbias Loss: 2.095141 Average df Loss 0.698447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.48it/s, Loss of this batch=2.023343, Avg Loss=0.001107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 162 Completed - Average unbias Loss: 2.091105 Average df Loss 0.699800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/200:  78%|███████▊  | 1428/1828 [00:45<00:12, 31.49it/s, Loss of this batch=2.737185, Avg Loss=0.001911] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 166/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.34it/s, Loss of this batch=2.058454, Avg Loss=0.001126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 166 Completed - Average unbias Loss: 2.092028 Average df Loss 0.698638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.38it/s, Loss of this batch=1.992673, Avg Loss=0.001090] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 168 Completed - Average unbias Loss: 2.091239 Average df Loss 0.699081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/200:  57%|█████▋    | 1040/1828 [00:32<00:25, 31.34it/s, Loss of this batch=2.880031, Avg Loss=0.002764]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 171/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.37it/s, Loss of this batch=2.014144, Avg Loss=0.001102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 171 Completed - Average unbias Loss: 2.093177 Average df Loss 0.694515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.44it/s, Loss of this batch=1.948924, Avg Loss=0.001066] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 173 Completed - Average unbias Loss: 2.092624 Average df Loss 0.691515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/200:  82%|████████▏ | 1504/1828 [00:47<00:10, 31.87it/s, Loss of this batch=2.083269, Avg Loss=0.001384] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 178/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.68it/s, Loss of this batch=1.971024, Avg Loss=0.001078] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 178 Completed - Average unbias Loss: 2.091944 Average df Loss 0.692954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.64it/s, Loss of this batch=1.990509, Avg Loss=0.001089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 179 Completed - Average unbias Loss: 2.090702 Average df Loss 0.689656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/200:  16%|█▌        | 292/1828 [00:09<00:48, 31.54it/s, Loss of this batch=3.144584, Avg Loss=0.010660]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 182/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.56it/s, Loss of this batch=1.982254, Avg Loss=0.001084] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 182 Completed - Average unbias Loss: 2.091408 Average df Loss 0.686068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.73it/s, Loss of this batch=1.989991, Avg Loss=0.001089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 184 Completed - Average unbias Loss: 2.090592 Average df Loss 0.686069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/200:  54%|█████▍    | 992/1828 [00:31<00:26, 32.06it/s, Loss of this batch=3.376830, Avg Loss=0.003397] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 188/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.50it/s, Loss of this batch=1.989967, Avg Loss=0.001089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 188 Completed - Average unbias Loss: 2.091814 Average df Loss 0.683796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.38it/s, Loss of this batch=1.989868, Avg Loss=0.001089] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 189 Completed - Average unbias Loss: 2.086990 Average df Loss 0.691497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190/200:  98%|█████████▊| 1783/1828 [00:57<00:01, 31.22it/s, Loss of this batch=2.093387, Avg Loss=0.001172] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 195/200: 100%|██████████| 1828/1828 [00:58<00:00, 31.35it/s, Loss of this batch=1.981653, Avg Loss=0.001084] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 195 Completed - Average unbias Loss: 2.088650 Average df Loss 0.686157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196/200: 100%|██████████| 1828/1828 [00:57<00:00, 31.53it/s, Loss of this batch=2.015147, Avg Loss=0.001102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 196 Completed - Average unbias Loss: 2.087490 Average df Loss 0.685858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197/200:  95%|█████████▍| 1732/1828 [00:55<00:03, 31.40it/s, Loss of this batch=2.619119, Avg Loss=0.001511] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "lora_model.train()\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_loss1 = 0.0\n",
    "    total_loss2 = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(traindataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    for tensor_stamp_train, tensor_mask_train, tensor_mask_gal, tensor_mask_annulus, tensor_area_annulus, features_df_train_tensor in progress_bar:\n",
    "\n",
    "        inputs = tensor_stamp_train.to(device)\n",
    "        mask_abnormal = tensor_mask_train.to(device) \n",
    "        mask_gal = tensor_mask_gal.to(device)\n",
    "        mask_annulus = tensor_mask_annulus.to(device)\n",
    "        area_annulus = tensor_area_annulus.to(device)\n",
    "        features = features_df_train_tensor.to(device)#'area', 'ref_id', 'zp'\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = lora_model(inputs) \n",
    "        #batch_size = inputs.shape[0]\n",
    "        batch_size = inputs.shape[0]\n",
    "        #get the flux\n",
    "        flux_raw_inputs = torch.sum(inputs * mask_gal * mask_abnormal, dim=[2, 3])\n",
    "        flux_raw_outputs = torch.sum(outputs * mask_gal * mask_abnormal, dim=[2, 3])\n",
    "        flux_annulus_inputs = torch.sum(inputs * mask_annulus, dim=[2, 3])\n",
    "        flux_annulus_outputs = torch.sum(outputs * mask_annulus, dim=[2, 3])\n",
    "        flux_gal_inputs = flux_raw_inputs - flux_annulus_inputs * features[:,0].unsqueeze(1) / area_annulus\n",
    "        flux_gal_outputs = flux_raw_outputs - flux_annulus_outputs * features[:,0].unsqueeze(1) / area_annulus\n",
    "        #get the calibrated flux.\n",
    "        #Note that here we just calculate the calibrated flux of the denoised image, we don't need to include the undenoised ones\n",
    "        flux_gal_calibrated_outputs = flux_gal_outputs * features[:,2].unsqueeze(1)\n",
    "        \n",
    "        loss1 = loss_unbias(flux_gal_inputs, flux_gal_outputs) / batch_size\n",
    "        loss2 = PairedDifferenceLoss(flux_gal_calibrated_outputs) / batch_size#(batch_size/2)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss1 = loss1.item()\n",
    "        current_loss2 = loss2.item()\n",
    "        total_loss1 += current_loss1\n",
    "        total_loss2 += current_loss2\n",
    "        num_batches += 1\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss of this batch': f'{current_loss1 + current_loss2:.6f}',\n",
    "            'Avg Loss': f'{(current_loss1+current_loss2)/num_batches:.6f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss1 = total_loss1 / num_batches\n",
    "    avg_loss2 = total_loss2 / num_batches\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Completed - Average unbias Loss: {avg_loss1:.6f} Average df Loss {avg_loss2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb31c60-ba2f-42e2-b50a-cc1d772cfeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d0ae3a-10da-48c6-8f69-018db238ed3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def background_annulus(data, mask, aperture_x, aperture_y, r_in=30, r_out=45):\n",
    "    \"\"\"Measure background in an annulus.\"\"\"\n",
    "    \n",
    "    masked_data = np.ma.array(data=data, mask=mask != 0)\n",
    "    masked_data = masked_data.filled(fill_value=0)\n",
    "\n",
    "    center = (aperture_x, aperture_y)\n",
    "    annulus_apertures = CircularAnnulus(center, r_in=r_in, r_out=r_out)\n",
    "    masks = annulus_apertures.to_mask(method='center')\n",
    "\n",
    "    cutout_data = masks.cutout(masked_data)\n",
    "\n",
    "    clip_annulus_array = sigma_clip(cutout_data[cutout_data != 0], sigma=3, maxiters=2)\n",
    "\n",
    "    S = pd.Series()\n",
    "    S['annulus_mean'] = np.ma.mean(clip_annulus_array)\n",
    "    S['annulus_median'] = np.ma.median(clip_annulus_array)\n",
    "    S['annulus_std'] = np.ma.std(clip_annulus_array)\n",
    "    S['annulus_samples'] = np.ma.count(clip_annulus_array)\n",
    "\n",
    "    return S\n",
    "\n",
    "def flux_elliptical(image, mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b):\n",
    "    \"\"\"Measure the flux withing an elliptical aperture.\"\"\"\n",
    "    \n",
    "    PIXEL_SCALE = 0.263\n",
    "    theta = -aperture_theta * np.pi / 180.\n",
    "    a = aperture_a / PIXEL_SCALE\n",
    "    b = aperture_b / PIXEL_SCALE\n",
    "\n",
    "    center = (aperture_x, aperture_y)\n",
    "    source_aperture = EllipticalAperture(center, a, b, theta)\n",
    "\n",
    "    xmask = mask != 0\n",
    "    raw_flux = aperture_photometry(image, source_aperture, mask=xmask)\n",
    "   \n",
    "    S = pd.Series()\n",
    "    S['raw_flux'] = float(raw_flux['aperture_sum'][0])\n",
    "    S['area'] = source_aperture.area\n",
    "    \n",
    "    return S\n",
    "\n",
    "def cal_calerror(sig_src,sig_zp,zp,f_src):\n",
    "    sig_cal = np.sqrt(sig_src**2 * sig_zp**2 + sig_src**2 * zp**2 + sig_zp**2 * f_src**2)\n",
    "    return sig_cal\n",
    "\n",
    "def cal_fcal(f_src,zp):\n",
    "    f_cal = f_src * zp\n",
    "    return f_cal\n",
    "\n",
    "def creat_stamps(image, sources):\n",
    "    y = math.floor(sources['aperture_x'].values[0])\n",
    "    x = math.floor(sources['aperture_y'].values[0])\n",
    "    #y = math.floor(sources['aperture_x'])\n",
    "    #x = math.floor(sources['aperture_y'])\n",
    "    x_start = max((x - cutout_size), 0)\n",
    "    x_end = min((x + cutout_size), image.shape[0])\n",
    "    y_start = max((y - cutout_size), 0)\n",
    "    y_end = min((y + cutout_size), image.shape[1])\n",
    "\n",
    "    stamps = image[x_start:x_end, y_start:y_end]\n",
    "    return stamps\n",
    "\n",
    "def photometry_oneimage(image, mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b):\n",
    "    \n",
    "    S1 = background_annulus(image, mask, aperture_x, aperture_y)\n",
    "    S2 = flux_elliptical(image, mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b)\n",
    "\n",
    "    flux_obs = S2['raw_flux'] - S2['area'] * S1['annulus_median']\n",
    "    return flux_obs, S1['annulus_std']\n",
    "\n",
    "def generate_mask(size_data, image):\n",
    "\n",
    "    num_sample = int(size_data[0] * size_data[1] * (1 - ratio))\n",
    "    mask = np.ones(size_data)\n",
    "    output = image\n",
    "\n",
    "    for ich in range(size_data[2]):\n",
    "        idy_msk = np.random.randint(0, size_data[0], num_sample)\n",
    "        idx_msk = np.random.randint(0, size_data[1], num_sample)\n",
    "\n",
    "        idy_neigh = np.random.randint(-size_window[0] // 2 + size_window[0] % 2, size_window[0] // 2 + size_window[0] % 2, num_sample)\n",
    "        idx_neigh = np.random.randint(-size_window[1] // 2 + size_window[1] % 2, size_window[1] // 2 + size_window[1] % 2, num_sample)\n",
    "\n",
    "        idy_msk_neigh = idy_msk + idy_neigh\n",
    "        idx_msk_neigh = idx_msk + idx_neigh\n",
    "\n",
    "        idy_msk_neigh = idy_msk_neigh + (idy_msk_neigh < 0) * size_data[0] - (idy_msk_neigh >= size_data[0]) * size_data[0]\n",
    "        idx_msk_neigh = idx_msk_neigh + (idx_msk_neigh < 0) * size_data[1] - (idx_msk_neigh >= size_data[1]) * size_data[1]\n",
    "\n",
    "        id_msk = (idy_msk, idx_msk, ich)\n",
    "        id_msk_neigh = (idy_msk_neigh, idx_msk_neigh, ich)\n",
    "\n",
    "        output[id_msk] = image[id_msk_neigh]\n",
    "        mask[id_msk] = 0.0\n",
    "\n",
    "    return output, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b14801f7-3853-471f-9d7f-df42a5e1db8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#write the function to calculate the flux by myself\n",
    "\n",
    "def background_annulus_jiefeng(data, mask, aperture_x, aperture_y, r_in=30, r_out=45):\n",
    "    \n",
    "    masked_data = np.ma.array(data=data, mask=mask != 0)\n",
    "    masked_data = masked_data.filled(fill_value=0)\n",
    "\n",
    "    center = (aperture_x, aperture_y)\n",
    "    annulus_apertures = CircularAnnulus(center, r_in=r_in, r_out=r_out)\n",
    "    masks = annulus_apertures.to_mask(method='center')\n",
    "    cutout_data = masks.cutout(masked_data)\n",
    "\n",
    "    clip_annulus_array = sigma_clip(cutout_data[cutout_data != 0], sigma=3, maxiters=2)\n",
    "\n",
    "    #background_annulus = np.ma.mean(clip_annulus_array)\n",
    "    #we use median here, in the dataset they use mdian\n",
    "    background_annulus = np.ma.median(clip_annulus_array)\n",
    "    return background_annulus\n",
    "\n",
    "def flux_elliptical_jiefeng(image, mask, aperture_x, aperture_y, aperture_theta, aperture_a, aperture_b):\n",
    "\n",
    "    image_shape = (cutout_size*2,cutout_size*2)\n",
    "    PIXEL_SCALE = 0.263\n",
    "    theta = -aperture_theta * np.pi / 180.\n",
    "    a = aperture_a / PIXEL_SCALE\n",
    "    b = aperture_b / PIXEL_SCALE\n",
    "\n",
    "    center = (aperture_x, aperture_y)\n",
    "    source_aperture = EllipticalAperture(center, a, b, theta)\n",
    "    mask_object = source_aperture.to_mask(method='exact')\n",
    "    mask_image_photutils_fractional = mask_object.to_image(shape=image_shape)\n",
    "    \n",
    "    xmask = mask != 0\n",
    "    image_good = image * (1 - xmask)\n",
    "    \n",
    "    raw_flux = np.sum(image_good * mask_image_photutils_fractional)#calculate by myself\n",
    "\n",
    "    background = background_annulus_jiefeng(image, mask, aperture_x, aperture_y)\n",
    "    \n",
    "    gal_flux = raw_flux - source_aperture.area * background\n",
    "    return gal_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729b4852-9cfa-4d19-8edb-586c9f742284",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63/1489632568.py:2: DeprecationWarning: `photutils.CircularAnnulus` is a deprecated alias for `photutils.aperture.CircularAnnulus` and will be removed in the future. Instead, please use `from photutils.aperture import CircularAnnulus` to silence this warning.\n",
      "  from photutils import CircularAnnulus, EllipticalAperture\n",
      "/tmp/ipykernel_63/1489632568.py:2: DeprecationWarning: `photutils.EllipticalAperture` is a deprecated alias for `photutils.aperture.EllipticalAperture` and will be removed in the future. Instead, please use `from photutils.aperture import EllipticalAperture` to silence this warning.\n",
      "  from photutils import CircularAnnulus, EllipticalAperture\n"
     ]
    }
   ],
   "source": [
    "from astropy.io import fits\n",
    "from photutils import CircularAnnulus, EllipticalAperture\n",
    "from astropy.stats import sigma_clip\n",
    "from photutils.aperture import aperture_photometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef91e427-dcd2-4e84-8809-57fc792c4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = shuffled_df_train[['area']].values\n",
    "#area = trainingset[['area']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddba9bb3-c954-4290-8538-e3a3af526c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.93264236]\n"
     ]
    }
   ],
   "source": [
    "num = 30\n",
    "raw = np.sum(shuffled_stamp_train[num] * shuffled_mask_train[num] * shuffled_mask_gal[num])\n",
    "bgk_mean = np.sum(shuffled_stamp_train[num] * shuffled_mask_train[num] * shuffled_mask_annulus[num]) / shuffled_area_annulus[num]\n",
    "gal = raw - bgk_mean * area[num]\n",
    "\n",
    "print(gal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6971f2ca-71e6-42b5-be76-bde1e61b1add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.893120548310137\n"
     ]
    }
   ],
   "source": [
    "row = shuffled_df_train.iloc[[num]]\n",
    "path = shuffled_df_train['path'][num]\n",
    "image = fits.getdata(path)\n",
    "mask = fits.getdata(path.replace('.fits', '.mask.fits'))\n",
    "\n",
    "cutout_size = 48\n",
    "aperture_x = cutout_size + row['aperture_x'].item() - math.floor(row['aperture_x'].item())\n",
    "aperture_y = cutout_size + row['aperture_y'].item() - math.floor(row['aperture_y'].item())\n",
    "\n",
    "stamp_image = creat_stamps(image, row)\n",
    "mask_cutout = creat_stamps(mask, row)\n",
    "\n",
    "gal_flux = flux_elliptical_jiefeng(stamp_image, mask_cutout, aperture_x, aperture_y,\n",
    "                                   row['aperture_theta'].item(),row['aperture_a'].item(),row['aperture_b'].item())\n",
    "print(gal_flux)\n",
    "#print(row['area'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f771fe8-bd3c-4439-bae8-5dc2b6441eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoise",
   "language": "python",
   "name": "denoise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
